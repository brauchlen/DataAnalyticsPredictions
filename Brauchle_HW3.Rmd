---
title: "Homework3"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE, results = "hide"}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
library(ggplot2)
library(grid)
library(gridExtra)
library(ISLR)
library(MASS)
library(class)
Weekly <- Weekly

```



```{r}
#a 
names(Weekly)
pairs(Weekly)


Weekly %>% group_by(Direction) %>% summarize(mean = mean(Volume))
Weekly %>% group_by(Direction) %>% summarize(mean = mean(Lag1))

p1 <- Weekly %>% ggplot(aes(Direction, Volume)) + geom_boxplot()
p2 <- Weekly %>% ggplot(aes(Direction, Lag1)) + geom_boxplot()

p3 <- Weekly %>% ggplot(aes(Year, Volume, color = Direction)) + geom_point()
p4 <- Weekly %>% ggplot(aes(Year, Lag1, color = Direction)) + geom_point()
grid.arrange(p1, p2, p3, p4)



```

(a) We can see that there does not appear to be much of a difference in the mean of Volume based on whether the market had a positive or negative return as the mean of Volume is 1.61 when Direction is "Down" and 1.55 when Direction is "Up." A similar pattern is seen for Lag1, which is also confirmed by looking at the boxplots. The scatterplot of Year against Volume shows that the standard devation and mean of Volume appears to increase with Year. Lag1 does not appear to have a relationship with Year, based on the scatterplot. Neither one of the scatterplots shows any relationships that these variables have with direction. The pairs plot shows that volume and year have a curvilinear relationship . There do not appear to be strong relationships among the rest of the variables, but there appears to be an outlier in Lag1. 



```{r}
#b
names(Weekly)
a <- table(Weekly$Direction)
a
log_model = glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family= "binomial") 
summary(log_model)
```

(b) Only Lag2 has a significant relationship with Direction (z = 2.18, p = 0.03 < 0.05). 

```{r}
#c
glm.probs = predict(log_model, type="response") 
glm.pred = rep("Down",1089) 
glm.pred[glm.probs >.5]=" Up"


table(glm.pred, Weekly$Direction )


correct = (54+557)/1089
correct

# correct when actually up
557/(557 + 48)

# correct when actually down
(54)/(54 + 430)
```

(c) The logistic regression model predicted 56.1% of all of the data's direction values correctly.  It was able to predict "up" values more accurately than "down" values with accuracy rates of 92.1% and 11.2%, respectively. This means that when the market actually did go up, the model predicted this accuraely 92.1% of the time, but only predicted the market going down when it actually did 11.2% of the time.

```{r}
#d 
names(Weekly)
train_set = Weekly %>% filter(Year <= 2008)


train_lm = glm(Direction ~ Lag2, data=train_set, family=binomial ) 
summary(train_lm)


test_set = Weekly %>% filter(Year > 2008)
#up up down down up up up down down na
test.probs = predict (train_lm, newdata = test_set, type="response")
test.pred = rep("Down", 104)
test.pred[test.probs > .5] = "Up"
table(test.pred, test_set$Direction)


correct = (9 + 56)/104
correct
```

(d) The logistic regression model has a 62.5% correct rate for the test data that was composed of 2009 and 2010 data. 

```{r}
#e

train_lda = lda(Direction ~ Lag2, data=train_set, family=binomial ) 
train_lda

lda.pred = predict (train_lda, test_set)
lda.class = lda.pred$class
table(lda.class, test_set$Direction)


correct = (9 + 56)/104
correct

# predicting up
56/(56+5)


#predicting down
9/(9+34)
```

(e) The LDA predicted 62.5% of the test data's directions correctly. When the market actually went down, the model predicted this 91.8% of the time, but only 20.9%  of "down" predictions were correct when the market truly went down. 

```{r}
#f
train_qda = qda(Direction ~ Lag2, data=train_set, family=binomial ) 
train_qda


qda.pred = predict (train_qda, test_set)
qda.class = qda.pred$class
table(qda.class, test_set$Direction)

correct = (0 + 61)/104
correct

```

(f) The QDA predicted 58.7% of the test data's directions correctly. 

```{r}
#g 
train.X = cbind(train_set$Lag2)
test.X = cbind(test_set$Lag2)
train.Direction = train_set$Direction

set.seed(1)
knn.pred = knn(train.X, test.X, train.Direction, k = 1)
table(knn.pred, test_set$Direction)

correct = (21 + 31)/104
correct
```

(g) The k-nearest neighbors method only predicted 50% of the test data's directions correctly. 


(h) LDA and logistic regression provided the best results, as both had an accuracy of 62.5%, followed by QDA at 58.7%. 

```{r}
#i
## logistic
train_lm = glm(Direction ~ Lag2 + log(Volume)*Year , data=train_set, family=binomial ) 
summary(train_lm)

test.probs = predict (train_lm, newdata = test_set, type="response")
test.pred = rep("Down", 104)
test.pred[test.probs > .5] = "Up"
table(test.pred, test_set$Direction)


correct = (18 + 43)/104
correct

# lda
train_lda = lda(Direction ~ Volume + Lag3 + Year, data=train_set, family=binomial ) 
train_lda


lda.pred = predict (train_lda, test_set)
lda.class = lda.pred$class
table(lda.class, test_set$Direction)

correct = (36 + 16)/104
correct

## qda
train_qda = qda(Direction ~ log(Volume)*Year, data=train_set, family=binomial ) 
train_qda


qda.pred = predict(train_qda, test_set)
qda.class = qda.pred$class
table(qda.class, test_set$Direction)

correct = (42+4)/104
correct

## knearest

train.X = cbind(sqrt(train_set$Volume),train_set$Lag1)
test.X = cbind(sqrt(test_set$Volume),test_set$Lag1)
train.Direction = train_set$Direction

set.seed(1)
knn.pred = knn(train.X, test.X, train.Direction, k = 3)
table(knn.pred, test_set$Direction)

correct = (22+32)/104
correct

```
(i) None of these models outperformed the logistic and LDA models from parts (d) - (e) on the test data. The model that worked the best was the logistic regression with predictors log(Volume), Lag2, and Year with an interaction for log(Volume) and Year. However, this was not a significant interaction and none of these predictors were significant. This model had an accuracy rate of 58.7%, followed by the k-nearest with predictors log(Volume) and Year with k = 3 at 51.9%. The QDA model with predictors log(Volume), Year, and their interaction had the worst accuracy rating of 44.2%. 